# Workflow Improvement: Merge Tasks & Enhanced Data Flow

## Overview

This implementation adds a sophisticated data flow system to the Offensive Security Platform that enables:
- **Merge tasks** for deduplicating and combining results from multiple security tools
- **Structured file organization** with separate directories for raw, parsed, and combined results
- **Relational database tracking** for subdomains, IPs, ports, and ASNs
- **Association tracking** to maintain relationships between discovered assets
- **Enhanced tool adapters** that automatically save results to organized file structures

## Key Features

### 1. Merge Task Type

Workflows can now include `merge` tasks that:
- Combine results from multiple source tasks
- Deduplicate entries based on a specified key
- Merge related data (e.g., combine multiple IPs for the same subdomain)
- Save results in multiple formats (JSON, plain text lists)

**Example:**
```python
WorkflowTask(
    task_id="merge_subdomains",
    name="Merge and Deduplicate Subdomains",
    task_type=TaskType.MERGE,
    merge_sources=["enum_amass", "enum_subfinder"],
    merge_field="subdomains",
    dedupe_key="name",
    merge_strategy="combine",  # Options: combine, replace, append
    depends_on=["enum_amass", "enum_subfinder"],
    priority=8
)
```

### 2. File Organization Structure

All scan results are now organized in a structured directory hierarchy:

```
data/scans/{domain}/
├── raw/                    # Raw tool output (exactly as the tool printed)
│   ├── amass/
│   │   └── output.json
│   ├── subfinder/
│   │   └── output.json
│   └── nmap/
│       └── output.xml
├── parsed/                 # Structured JSON files
│   ├── amass/
│   │   └── results.json
│   ├── subfinder/
│   │   └── results.json
│   └── nmap/
│       └── results.json
├── lists/                  # Plain text lists (one entry per line)
│   ├── subdomains.txt
│   ├── ips.txt
│   └── asns.txt
└── final/                  # Combined/aggregated results
    └── subdomains.json     # Merged data with full associations
```

### 3. Data Structure

The combined `subdomains.json` file contains enriched data:

```json
[
  {
    "name": "api.example.com",
    "ips": ["1.2.3.4", "5.6.7.8"],
    "asns": ["AS12345"],
    "ports": {
      "80": "http",
      "443": "https Apache 2.4.41",
      "8080": "http-proxy"
    },
    "source": "amass"
  }
]
```

### 4. Enhanced Tool Adapters

#### Amass Adapter
- Parses ASNs, IPs, and subdomains from JSON output
- Automatically merges duplicate subdomains with different IPs
- Saves raw and parsed results to appropriate directories

#### Subfinder Adapter
- Includes `-ip` flag by default to resolve IPs
- Parses subdomains with associated IPs
- Saves raw and parsed results automatically

#### Nmap Adapter
- Reads IPs from `ips.txt` (generated by merge tasks)
- Scans all discovered IPs
- Updates `subdomains.json` with port information
- Maintains IP-to-subdomain associations

### 5. Database Schema

New relational tables track all discovered assets:

```
subdomains
├── id (PK)
├── scan_id (FK)
├── name
├── source
└── discovered_at

ips
├── id (PK)
├── scan_id (FK)
├── address
└── discovered_at

ports
├── id (PK)
├── ip_id (FK)
├── scan_id (FK)
├── port_number
├── protocol
├── service
├── version
└── discovered_at

asns
├── id (PK)
├── scan_id (FK)
├── asn_number
├── organization
└── discovered_at
```

**Relationships:**
- Subdomain ↔ IP (many-to-many)
- Subdomain ↔ ASN (many-to-many)
- IP → Port (one-to-many)

## Workflow Examples

### Example 1: Subdomain Enumeration with Merge

```python
from app.workflows.schemas import WorkflowDefinition, WorkflowTask, TaskType

workflow = WorkflowDefinition(
    workflow_id="subdomain_enum_example.com",
    name="Subdomain Enumeration",
    target="example.com",
    tasks=[
        # Step 1: Run Amass
        WorkflowTask(
            task_id="enum_amass",
            name="Amass Enumeration",
            tool="amass",
            parameters={"domain": "example.com", "passive": True},
            priority=10
        ),

        # Step 2: Run Subfinder
        WorkflowTask(
            task_id="enum_subfinder",
            name="Subfinder Enumeration",
            tool="subfinder",
            parameters={"domain": "example.com", "resolve": True},
            priority=9
        ),

        # Step 3: Merge results
        WorkflowTask(
            task_id="merge_subdomains",
            name="Merge Results",
            task_type=TaskType.MERGE,
            merge_sources=["enum_amass", "enum_subfinder"],
            merge_field="subdomains",
            dedupe_key="name",
            merge_strategy="combine",
            depends_on=["enum_amass", "enum_subfinder"],
            priority=8
        ),

        # Step 4: Scan ports
        WorkflowTask(
            task_id="scan_ports",
            name="Port Scanning",
            tool="nmap",
            parameters={"domain": "example.com"},  # Uses ips.txt
            depends_on=["merge_subdomains"],
            priority=7
        )
    ]
)
```

### Example 2: Full Web Application Scan

The updated `web_app_scan` workflow now includes:
1. Amass enumeration (with ASNs and IPs)
2. Subfinder enumeration (with IP resolution)
3. Merge task (deduplication and IP combining)
4. Port scanning (using merged IPs)
5. HTTP probing (using merged subdomains)
6. Directory fuzzing
7. Vulnerability scanning
8. Exploitation testing

## Migration

### Migrate Existing Scans

To migrate existing scan data to the new database schema:

```python
from app.core.database import init_database, migrate_existing_scans

# Initialize database with new tables
init_database()

# Migrate existing scan data
migrate_existing_scans()
```

The migration function will:
- Read existing scan results from the database
- Parse JSON data from task outputs
- Populate new relational tables
- Preserve all historical data

## Usage

### Running Updated Workflows

```python
from app.workflows.prebuilt.subdomain_enum import create_subdomain_enum_workflow
from app.workflows.engine import WorkflowWorker

# Create workflow
workflow = create_subdomain_enum_workflow("example.com")

# Execute
worker = WorkflowWorker(workflow, user_id=1)
worker.start()
```

### Accessing Results

**From Files:**
```python
import json
from pathlib import Path

# Read combined subdomains with all associations
domain = "example.com"
subdomains_file = Path("data/scans") / domain / "final" / "subdomains.json"

with open(subdomains_file, 'r') as f:
    subdomains = json.load(f)

# Each subdomain has: name, ips, asns, ports (if scanned)
for subdomain in subdomains:
    print(f"{subdomain['name']}: {subdomain['ips']}")
```

**From Database:**
```python
from app.core.database import SessionLocal, Subdomain, IP, Port

db = SessionLocal()

# Get all subdomains for a scan with their IPs and ports
scan_id = 1
subdomains = db.query(Subdomain).filter(Subdomain.scan_id == scan_id).all()

for subdomain in subdomains:
    print(f"Subdomain: {subdomain.name}")
    for ip in subdomain.ips:
        print(f"  IP: {ip.address}")
        for port in ip.ports:
            print(f"    Port {port.port_number}: {port.service}")
```

## Merge Strategies

### 1. Combine (Default)
Merges data for duplicate entries:
- Same subdomain from different sources → combine IPs and ASNs
- Example: `api.example.com` found by both Amass and Subfinder with different IPs

### 2. Replace
Last source wins:
- Overwrites previous data with latest
- Useful when newer data is more accurate

### 3. Append
Keeps all entries:
- Creates unique entries for each source
- Useful for tracking which tool found what

## Benefits

1. **No Data Loss**: All tools save both raw and parsed output
2. **Easy Analysis**: Plain text lists for quick review
3. **Full Context**: Combined JSON maintains all associations
4. **Database Tracking**: Query relationships across scans
5. **Tool Integration**: Subsequent tools use merged results automatically
6. **Reproducibility**: Raw outputs preserved for re-parsing

## Architecture Notes

### Workflow Engine Changes

- `WorkflowWorker._execute_task()` now checks task type
- New method `_execute_merge_task()` handles merge operations
- Merge tasks create directory structures automatically
- File I/O errors don't break the workflow (silent failures)

### Tool Adapter Pattern

All enhanced adapters follow this pattern:
1. Execute tool and capture output
2. Parse output to structured format
3. Save raw output to `raw/{tool}/`
4. Save parsed output to `parsed/{tool}/`
5. Return structured data to workflow engine

### Parameter Substitution

Merge task outputs can be referenced in subsequent tasks:
```python
parameters={
    "urls": "${merge_subdomains.merged_data}"  # Access merged data
}
```

## Testing

To test the new workflow:

1. Run the subdomain enumeration workflow:
   ```bash
   python main.py
   # From UI: Select "Subdomain Enumeration" workflow
   ```

2. Check file outputs:
   ```bash
   ls -R data/scans/example.com/
   cat data/scans/example.com/lists/subdomains.txt
   cat data/scans/example.com/final/subdomains.json
   ```

3. Query database:
   ```python
   from app.core.database import SessionLocal, Subdomain
   db = SessionLocal()
   subdomains = db.query(Subdomain).all()
   print(f"Found {len(subdomains)} subdomains in database")
   ```

## Future Enhancements

Potential improvements:
- Merge strategies for other data types (vulnerabilities, findings)
- Diff functionality to compare scan results over time
- Export to other formats (CSV, XML, Excel)
- Real-time streaming updates to files during scan
- Visualization of subdomain/IP/port relationships

## Files Modified

- `app/workflows/schemas.py` - Added TaskType enum, merge task fields
- `app/workflows/engine.py` - Added merge task execution logic
- `app/tools/adapters/amass_adapter.py` - Enhanced parsing, file saving
- `app/tools/adapters/subfinder_adapter.py` - Enhanced parsing, file saving
- `app/tools/adapters/nmap_adapter.py` - IP file reading, JSON updating
- `app/core/database.py` - New tables and migration logic
- `app/workflows/prebuilt/subdomain_enum.py` - Updated with merge task
- `app/workflows/prebuilt/web_app_scan.py` - Updated with merge task

## References

- Original CLAUDE.md: Project architecture and tool adapter pattern
- app/workflows/schemas.py: Schema definitions and validators
- app/workflows/engine.py: Workflow execution logic
