# Advanced Reconnaissance & Exploitation Workflow Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement a comprehensive automated penetration testing workflow that performs deep reconnaissance, vulnerability discovery, and automated exploitation with intelligent deduplication and result aggregation.

**Architecture:** Multi-stage workflow engine with result merging, file-based persistence, exploit database integration, web crawling, and automated exploitation. Uses existing BaseTool pattern, WorkflowEngine with TaskType.MERGE, and new custom task processors for complex operations (crawling, exploit lookup, manual test coordination).

**Tech Stack:** Python 3.10+, PyQt5, SQLAlchemy, Pydantic, subprocess management, JSON file I/O, HTTP clients (requests/httpx), BeautifulSoup for HTML parsing, CVE/ExploitDB API integration

---

## Phase 1: Missing Tool Adapters

### Task 1: Sublist3r Adapter

**Files:**
- Create: `app/tools/adapters/sublist3r_adapter.py`
- Modify: `app/tools/registry.py:26-44`

**Step 1: Write failing test for Sublist3r adapter**

Create: `tests/tools/test_sublist3r_adapter.py`

```python
import pytest
from app.tools.adapters.sublist3r_adapter import Sublist3rAdapter
from app.tools.base import ToolCategory

def test_sublist3r_metadata():
    adapter = Sublist3rAdapter()
    metadata = adapter.get_metadata()
    assert metadata.name == "sublist3r"
    assert metadata.category == ToolCategory.RECONNAISSANCE
    assert metadata.executable == "sublist3r"

def test_sublist3r_validate_parameters():
    adapter = Sublist3rAdapter()
    assert adapter.validate_parameters({"domain": "example.com"}) == True
    assert adapter.validate_parameters({}) == False

def test_sublist3r_build_command():
    adapter = Sublist3rAdapter()
    cmd = adapter.build_command({"domain": "example.com"})
    assert cmd == ["sublist3r", "-d", "example.com", "-o", "/dev/stdout"]

def test_sublist3r_parse_output():
    adapter = Sublist3rAdapter()
    output = "www.example.com\nmail.example.com\nftp.example.com\n"
    result = adapter.parse_output(output, "", 0)
    assert "subdomains" in result
    assert len(result["subdomains"]) == 3
    assert result["subdomains"][0]["name"] == "www.example.com"
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/tools/test_sublist3r_adapter.py -v`

Expected: FAIL with "ModuleNotFoundError: No module named 'app.tools.adapters.sublist3r_adapter'"

**Step 3: Write minimal Sublist3r adapter implementation**

Create: `app/tools/adapters/sublist3r_adapter.py`

```python
"""Sublist3r tool adapter"""
from app.tools.base import BaseTool, ToolMetadata, ToolCategory
from typing import Dict, Any, List
import json
from pathlib import Path

class Sublist3rAdapter(BaseTool):

    def get_metadata(self) -> ToolMetadata:
        return ToolMetadata(
            name="sublist3r",
            category=ToolCategory.RECONNAISSANCE,
            description="Fast subdomain enumeration using multiple search engines",
            executable="sublist3r",
            requires_root=False,
            default_timeout=600
        )

    def validate_parameters(self, params: Dict[str, Any]) -> bool:
        return "domain" in params

    def build_command(self, params: Dict[str, Any]) -> List[str]:
        cmd = [
            self.metadata.executable,
            "-d", params["domain"],
            "-o", "/dev/stdout"  # Output to stdout
        ]

        # Enable brute force if requested
        if params.get("brute"):
            cmd.append("-b")

        # Specify ports for brute force
        if params.get("ports"):
            cmd.extend(["-p", params["ports"]])

        # Verbose mode
        if params.get("verbose"):
            cmd.append("-v")

        return cmd

    def parse_output(self, output: str, stderr: str, return_code: int) -> Dict[str, Any]:
        """Parse Sublist3r text output into structured subdomain list"""
        subdomains_list = []

        for line in output.strip().split('\n'):
            line = line.strip()
            if line and not line.startswith('['):  # Filter out log lines
                subdomains_list.append({
                    "name": line,
                    "ips": [],  # Sublist3r doesn't provide IPs by default
                    "source": "sublist3r"
                })

        # Save results
        if subdomains_list:
            domain = subdomains_list[0]["name"].split(".")[-2:]
            domain = ".".join(domain) if len(domain) >= 2 else "unknown"
            self._save_results(domain, output, subdomains_list)

        return {
            "subdomains": subdomains_list,
            "count": len(subdomains_list)
        }

    def _save_results(self, domain: str, raw_output: str, parsed_data: List[Dict]):
        """Save raw and parsed results to files"""
        try:
            base_dir = Path("data/scans") / domain
            raw_dir = base_dir / "raw" / "sublist3r"
            parsed_dir = base_dir / "parsed" / "sublist3r"

            raw_dir.mkdir(parents=True, exist_ok=True)
            parsed_dir.mkdir(parents=True, exist_ok=True)

            # Save raw output
            raw_file = raw_dir / "output.txt"
            with open(raw_file, 'w') as f:
                f.write(raw_output)

            # Save parsed output
            parsed_file = parsed_dir / "results.json"
            with open(parsed_file, 'w') as f:
                json.dump(parsed_data, f, indent=2)

        except Exception:
            pass  # Silently fail if file saving fails
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/tools/test_sublist3r_adapter.py -v`

Expected: PASS (all 4 tests)

**Step 5: Register Sublist3r in ToolRegistry**

Modify: `app/tools/registry.py`

```python
# Add import at top
from app.tools.adapters.sublist3r_adapter import Sublist3rAdapter

# In _register_tools method, add after line 30:
self.register("sublist3r", Sublist3rAdapter)
```

**Step 6: Commit**

```bash
git add app/tools/adapters/sublist3r_adapter.py tests/tools/test_sublist3r_adapter.py app/tools/registry.py
git commit -m "feat: add Sublist3r tool adapter for subdomain enumeration

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 2: Masscan Adapter

**Files:**
- Create: `app/tools/adapters/masscan_adapter.py`
- Modify: `app/tools/registry.py`

**Step 1: Write failing test for Masscan adapter**

Create: `tests/tools/test_masscan_adapter.py`

```python
import pytest
import json
from app.tools.adapters.masscan_adapter import MasscanAdapter
from app.tools.base import ToolCategory

def test_masscan_metadata():
    adapter = MasscanAdapter()
    metadata = adapter.get_metadata()
    assert metadata.name == "masscan"
    assert metadata.category == ToolCategory.SCANNING
    assert metadata.executable == "masscan"
    assert metadata.requires_root == True

def test_masscan_validate_parameters():
    adapter = MasscanAdapter()
    assert adapter.validate_parameters({"targets": ["192.168.1.1"], "ports": "80,443"}) == True
    assert adapter.validate_parameters({"targets": []}) == False

def test_masscan_build_command():
    adapter = MasscanAdapter()
    cmd = adapter.build_command({
        "targets": ["192.168.1.1", "192.168.1.2"],
        "ports": "80,443,8080",
        "rate": 10000
    })
    assert "masscan" in cmd
    assert "192.168.1.1,192.168.1.2" in cmd
    assert "-p80,443,8080" in cmd
    assert "--rate" in cmd
    assert "10000" in cmd

def test_masscan_parse_json_output():
    adapter = MasscanAdapter()
    json_output = """
    { "ip": "192.168.1.1", "timestamp": "1234567890", "ports": [ {"port": 80, "proto": "tcp", "status": "open", "reason": "syn-ack", "ttl": 64} ] }
    { "ip": "192.168.1.1", "timestamp": "1234567891", "ports": [ {"port": 443, "proto": "tcp", "status": "open", "reason": "syn-ack", "ttl": 64} ] }
    """
    result = adapter.parse_output(json_output, "", 0)
    assert "hosts" in result
    assert len(result["hosts"]) == 1
    assert result["hosts"][0]["ip"] == "192.168.1.1"
    assert len(result["hosts"][0]["ports"]) == 2
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/tools/test_masscan_adapter.py -v`

Expected: FAIL with "ModuleNotFoundError"

**Step 3: Write Masscan adapter implementation**

Create: `app/tools/adapters/masscan_adapter.py`

```python
"""Masscan tool adapter - high-speed port scanner"""
from app.tools.base import BaseTool, ToolMetadata, ToolCategory
from typing import Dict, Any, List
import json
from pathlib import Path
import tempfile
import os

class MasscanAdapter(BaseTool):

    def get_metadata(self) -> ToolMetadata:
        return ToolMetadata(
            name="masscan",
            category=ToolCategory.SCANNING,
            description="Ultra-fast TCP port scanner for large-scale networks",
            executable="masscan",
            requires_root=True,  # Masscan requires root for raw sockets
            default_timeout=1800  # 30 minutes for large scans
        )

    def validate_parameters(self, params: Dict[str, Any]) -> bool:
        """Validate that targets and ports are provided"""
        if "targets" not in params or not params["targets"]:
            return False
        if "ports" not in params and "port_range" not in params:
            # Default to common ports if not specified
            params["ports"] = "80,443,8080,8443,22,21,25,3306,3389"
        return True

    def build_command(self, params: Dict[str, Any]) -> List[str]:
        """Build masscan command with JSON output"""

        # Masscan requires targets as comma-separated or from file
        targets = params.get("targets", [])
        if isinstance(targets, list):
            target_str = ",".join(targets)
        else:
            target_str = targets

        # Build port specification
        ports = params.get("ports", "80,443")
        if isinstance(ports, list):
            ports = ",".join(map(str, ports))

        cmd = [
            self.metadata.executable,
            target_str,
            f"-p{ports}",
            "-oJ", "-",  # JSON output to stdout
            "--rate", str(params.get("rate", 10000))  # Default 10k packets/sec
        ]

        # Add optional parameters
        if params.get("banners"):
            cmd.append("--banners")

        if params.get("ping"):
            cmd.append("--ping")

        if params.get("exclude"):
            cmd.extend(["--exclude", params["exclude"]])

        return cmd

    def parse_output(self, output: str, stderr: str, return_code: int) -> Dict[str, Any]:
        """Parse masscan JSON output into structured host/port data"""
        hosts_dict = {}

        for line in output.strip().split('\n'):
            if not line or line.startswith('#'):
                continue

            try:
                data = json.loads(line.rstrip(','))

                ip = data.get("ip")
                if not ip:
                    continue

                # Initialize host if not seen
                if ip not in hosts_dict:
                    hosts_dict[ip] = {
                        "ip": ip,
                        "ports": [],
                        "services": []
                    }

                # Extract port information
                if "ports" in data:
                    for port_info in data["ports"]:
                        port_entry = {
                            "port": port_info.get("port"),
                            "protocol": port_info.get("proto", "tcp"),
                            "state": port_info.get("status", "open"),
                            "service": None  # Masscan doesn't detect service names
                        }

                        # Add banner if available
                        if "service" in port_info:
                            port_entry["banner"] = port_info["service"].get("banner", "")

                        hosts_dict[ip]["ports"].append(port_entry)

            except json.JSONDecodeError:
                continue
            except Exception:
                continue

        hosts_list = list(hosts_dict.values())

        # Save results
        if hosts_list:
            self._save_results(hosts_list, output)

        return {
            "hosts": hosts_list,
            "total_hosts": len(hosts_list),
            "total_ports": sum(len(h["ports"]) for h in hosts_list)
        }

    def _save_results(self, parsed_data: List[Dict], raw_output: str):
        """Save masscan results to files"""
        try:
            base_dir = Path("data/scans/masscan")
            raw_dir = base_dir / "raw"
            parsed_dir = base_dir / "parsed"

            raw_dir.mkdir(parents=True, exist_ok=True)
            parsed_dir.mkdir(parents=True, exist_ok=True)

            # Save raw JSON output
            import time
            timestamp = int(time.time())
            raw_file = raw_dir / f"scan_{timestamp}.json"
            with open(raw_file, 'w') as f:
                f.write(raw_output)

            # Save parsed output
            parsed_file = parsed_dir / f"results_{timestamp}.json"
            with open(parsed_file, 'w') as f:
                json.dump(parsed_data, f, indent=2)

        except Exception:
            pass
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/tools/test_masscan_adapter.py -v`

Expected: PASS

**Step 5: Register Masscan in ToolRegistry**

Modify: `app/tools/registry.py`

```python
# Add import
from app.tools.adapters.masscan_adapter import MasscanAdapter

# In _register_tools, add:
self.register("masscan", MasscanAdapter)
```

**Step 6: Commit**

```bash
git add app/tools/adapters/masscan_adapter.py tests/tools/test_masscan_adapter.py app/tools/registry.py
git commit -m "feat: add Masscan adapter for high-speed port scanning

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Phase 2: Result Utilities and File Handlers

### Task 3: Deduplication and List Utilities

**Files:**
- Create: `app/utils/result_utils.py`
- Create: `tests/utils/test_result_utils.py`

**Step 1: Write failing test for result utilities**

Create: `tests/utils/test_result_utils.py`

```python
import pytest
from app.utils.result_utils import (
    deduplicate_subdomains,
    merge_subdomain_lists,
    save_list_to_file,
    load_list_from_file,
    extract_ips_from_results
)

def test_deduplicate_subdomains():
    subdomains = [
        {"name": "www.example.com", "ips": ["1.1.1.1"]},
        {"name": "mail.example.com", "ips": ["2.2.2.2"]},
        {"name": "www.example.com", "ips": ["3.3.3.3"]},  # Duplicate
    ]

    result = deduplicate_subdomains(subdomains, merge_ips=True)
    assert len(result) == 2

    # Check that IPs were merged for www.example.com
    www_entry = next(s for s in result if s["name"] == "www.example.com")
    assert set(www_entry["ips"]) == {"1.1.1.1", "3.3.3.3"}

def test_merge_subdomain_lists():
    list1 = [{"name": "www.example.com", "ips": ["1.1.1.1"], "source": "amass"}]
    list2 = [{"name": "mail.example.com", "ips": ["2.2.2.2"], "source": "subfinder"}]
    list3 = [{"name": "www.example.com", "ips": ["3.3.3.3"], "source": "sublist3r"}]

    result = merge_subdomain_lists([list1, list2, list3])
    assert len(result) == 2

    www_entry = next(s for s in result if s["name"] == "www.example.com")
    assert len(www_entry["ips"]) == 2
    assert "amass,sublist3r" in www_entry["source"]

def test_save_and_load_list(tmp_path):
    items = ["www.example.com", "mail.example.com", "ftp.example.com"]
    filepath = tmp_path / "test_list.txt"

    save_list_to_file(items, filepath)
    assert filepath.exists()

    loaded = load_list_from_file(filepath)
    assert loaded == items

def test_extract_ips_from_results():
    results = {
        "hosts": [
            {"ip": "1.1.1.1", "ports": [{"port": 80}]},
            {"ip": "2.2.2.2", "ports": [{"port": 443}]}
        ]
    }

    ips = extract_ips_from_results(results)
    assert set(ips) == {"1.1.1.1", "2.2.2.2"}
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/utils/test_result_utils.py -v`

Expected: FAIL with "ModuleNotFoundError"

**Step 3: Write result utilities implementation**

Create: `app/utils/__init__.py` (empty file)

Create: `app/utils/result_utils.py`

```python
"""Utilities for result processing, deduplication, and file I/O"""
from typing import List, Dict, Any, Set
from pathlib import Path
import json

def deduplicate_subdomains(subdomains: List[Dict[str, Any]], merge_ips: bool = True) -> List[Dict[str, Any]]:
    """
    Deduplicate subdomain list by name, optionally merging IP addresses

    Args:
        subdomains: List of subdomain dictionaries with 'name' and 'ips' keys
        merge_ips: If True, merge IP lists for duplicate subdomains

    Returns:
        Deduplicated list of subdomains
    """
    seen = {}

    for subdomain in subdomains:
        name = subdomain.get("name")
        if not name:
            continue

        if name in seen:
            if merge_ips and "ips" in subdomain:
                # Merge IP addresses
                existing_ips = set(seen[name].get("ips", []))
                new_ips = set(subdomain.get("ips", []))
                seen[name]["ips"] = list(existing_ips | new_ips)

                # Merge sources
                existing_source = seen[name].get("source", "")
                new_source = subdomain.get("source", "")
                if new_source and new_source not in existing_source:
                    seen[name]["source"] = f"{existing_source},{new_source}" if existing_source else new_source
        else:
            seen[name] = subdomain.copy()

    return list(seen.values())

def merge_subdomain_lists(lists: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """
    Merge multiple subdomain lists with deduplication

    Args:
        lists: List of subdomain lists to merge

    Returns:
        Merged and deduplicated subdomain list
    """
    combined = []
    for sublist in lists:
        combined.extend(sublist)

    return deduplicate_subdomains(combined, merge_ips=True)

def save_list_to_file(items: List[str], filepath: Path, append: bool = False):
    """
    Save list of strings to text file (one per line)

    Args:
        items: List of strings to save
        filepath: Path to output file
        append: If True, append to existing file
    """
    filepath = Path(filepath)
    filepath.parent.mkdir(parents=True, exist_ok=True)

    mode = 'a' if append else 'w'
    with open(filepath, mode) as f:
        for item in items:
            f.write(f"{item}\n")

def load_list_from_file(filepath: Path) -> List[str]:
    """
    Load list of strings from text file

    Args:
        filepath: Path to input file

    Returns:
        List of strings (one per line, stripped)
    """
    filepath = Path(filepath)
    if not filepath.exists():
        return []

    with open(filepath, 'r') as f:
        return [line.strip() for line in f if line.strip()]

def extract_ips_from_results(results: Dict[str, Any], key: str = "hosts") -> List[str]:
    """
    Extract IP addresses from scan results

    Args:
        results: Result dictionary from tool execution
        key: Key containing host data (default: "hosts")

    Returns:
        List of unique IP addresses
    """
    ips = set()

    if key in results:
        for host in results[key]:
            if "ip" in host:
                ips.add(host["ip"])

    return list(ips)

def extract_subdomains_from_results(results: Dict[str, Any], key: str = "subdomains") -> List[str]:
    """
    Extract subdomain names from scan results

    Args:
        results: Result dictionary from tool execution
        key: Key containing subdomain data

    Returns:
        List of subdomain names
    """
    subdomains = []

    if key in results:
        for item in results[key]:
            if isinstance(item, dict) and "name" in item:
                subdomains.append(item["name"])
            elif isinstance(item, str):
                subdomains.append(item)

    return subdomains
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/utils/test_result_utils.py -v`

Expected: PASS

**Step 5: Commit**

```bash
git add app/utils/ tests/utils/
git commit -m "feat: add result utilities for deduplication and file I/O

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 4: File Output Task Processor

**Files:**
- Create: `app/workflows/processors/__init__.py`
- Create: `app/workflows/processors/file_output.py`
- Create: `tests/workflows/test_file_output_processor.py`
- Modify: `app/workflows/schemas.py` (add FILE_OUTPUT task type)

**Step 1: Write failing test for file output processor**

Create: `tests/workflows/test_file_output_processor.py`

```python
import pytest
from pathlib import Path
from app.workflows.processors.file_output import FileOutputProcessor
from app.workflows.schemas import WorkflowTask, TaskType

def test_file_output_processor_subdomains(tmp_path):
    """Test saving subdomain list to file"""
    processor = FileOutputProcessor()

    task = WorkflowTask(
        task_id="save_subdomains",
        name="Save Subdomains",
        task_type=TaskType.FILE_OUTPUT,
        parameters={
            "source_task": "enum_amass",
            "source_field": "subdomains",
            "output_file": str(tmp_path / "subdomains.txt"),
            "extract_field": "name"
        }
    )

    previous_results = {
        "enum_amass": {
            "subdomains": [
                {"name": "www.example.com", "ips": ["1.1.1.1"]},
                {"name": "mail.example.com", "ips": ["2.2.2.2"]}
            ]
        }
    }

    result = processor.execute(task, previous_results)

    assert result["success"] == True
    assert (tmp_path / "subdomains.txt").exists()

    lines = (tmp_path / "subdomains.txt").read_text().strip().split('\n')
    assert len(lines) == 2
    assert "www.example.com" in lines

def test_file_output_processor_ips(tmp_path):
    """Test saving IP list to file"""
    processor = FileOutputProcessor()

    task = WorkflowTask(
        task_id="save_ips",
        name="Save IPs",
        task_type=TaskType.FILE_OUTPUT,
        parameters={
            "source_task": "scan_masscan",
            "source_field": "hosts",
            "output_file": str(tmp_path / "ips.txt"),
            "extract_field": "ip"
        }
    )

    previous_results = {
        "scan_masscan": {
            "hosts": [
                {"ip": "1.1.1.1", "ports": []},
                {"ip": "2.2.2.2", "ports": []}
            ]
        }
    }

    result = processor.execute(task, previous_results)

    assert result["success"] == True
    lines = (tmp_path / "ips.txt").read_text().strip().split('\n')
    assert set(lines) == {"1.1.1.1", "2.2.2.2"}
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/workflows/test_file_output_processor.py -v`

Expected: FAIL

**Step 3: Update schemas to add FILE_OUTPUT task type**

Modify: `app/workflows/schemas.py` at line 49

```python
class TaskType(str, Enum):
    """Types of tasks in a workflow"""
    TOOL = "tool"  # Execute a security tool
    MERGE = "merge"  # Merge/deduplicate results from previous tasks
    FILE_OUTPUT = "file_output"  # Save results to file
    WEB_CRAWL = "web_crawl"  # Crawl websites for input fields
    EXPLOIT_LOOKUP = "exploit_lookup"  # Look up exploits for vulnerabilities
    JSON_AGGREGATE = "json_aggregate"  # Aggregate results into final JSON
```

**Step 4: Write file output processor**

Create: `app/workflows/processors/__init__.py` (empty)

Create: `app/workflows/processors/file_output.py`

```python
"""File output processor for saving workflow results to files"""
from typing import Dict, Any, List
from pathlib import Path
import json
from app.utils.result_utils import save_list_to_file

class FileOutputProcessor:
    """Processor for saving workflow results to text files"""

    def execute(self, task, previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute file output task

        Args:
            task: WorkflowTask with parameters:
                - source_task: Task ID to get data from
                - source_field: Field in source task results
                - output_file: Path to output file
                - extract_field: Optional field to extract from objects (e.g., "name" or "ip")
                - format: Optional format ("txt" or "json", default: "txt")
            previous_results: Dictionary of previous task results

        Returns:
            Result dictionary with success status and file path
        """
        try:
            params = task.parameters

            # Get source data
            source_task = params.get("source_task")
            source_field = params.get("source_field")

            if source_task not in previous_results:
                return {
                    "success": False,
                    "error": f"Source task '{source_task}' not found in previous results"
                }

            source_data = previous_results[source_task]

            if source_field not in source_data:
                return {
                    "success": False,
                    "error": f"Field '{source_field}' not found in source task results"
                }

            data = source_data[source_field]

            # Extract field if specified (e.g., extract "name" from list of dicts)
            extract_field = params.get("extract_field")
            if extract_field:
                if isinstance(data, list):
                    data = [
                        item[extract_field] if isinstance(item, dict) else item
                        for item in data
                        if (isinstance(item, dict) and extract_field in item) or not isinstance(item, dict)
                    ]

            # Ensure data is a list
            if not isinstance(data, list):
                data = [data]

            # Get output file path
            output_file = Path(params.get("output_file"))
            output_format = params.get("format", "txt")

            # Save based on format
            if output_format == "json":
                output_file.parent.mkdir(parents=True, exist_ok=True)
                with open(output_file, 'w') as f:
                    json.dump(data, f, indent=2)
            else:  # txt format
                # Convert all items to strings
                string_items = [str(item) for item in data]
                save_list_to_file(string_items, output_file)

            return {
                "success": True,
                "output_file": str(output_file),
                "items_written": len(data)
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
```

**Step 5: Run test to verify it passes**

Run: `pytest tests/workflows/test_file_output_processor.py -v`

Expected: PASS

**Step 6: Commit**

```bash
git add app/workflows/processors/ app/workflows/schemas.py tests/workflows/test_file_output_processor.py
git commit -m "feat: add file output processor for saving workflow results

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Phase 3: Advanced Processors

### Task 5: Web Crawler for Input Field Detection

**Files:**
- Create: `app/workflows/processors/web_crawler.py`
- Create: `tests/workflows/test_web_crawler.py`

**Step 1: Write failing test for web crawler**

Create: `tests/workflows/test_web_crawler.py`

```python
import pytest
from app.workflows.processors.web_crawler import WebCrawlerProcessor
from app.workflows.schemas import WorkflowTask, TaskType

def test_web_crawler_parse_html():
    """Test HTML parsing for input fields"""
    processor = WebCrawlerProcessor()

    html_content = """
    <html>
    <body>
        <form action="/login" method="post">
            <input type="text" name="username" />
            <input type="password" name="password" />
            <input type="submit" value="Login" />
        </form>
        <form action="/search">
            <input type="text" name="q" />
        </form>
    </body>
    </html>
    """

    forms = processor._parse_forms_from_html(html_content, "http://example.com/page")

    assert len(forms) == 2
    assert forms[0]["action"] == "http://example.com/login"
    assert forms[0]["method"] == "post"
    assert len(forms[0]["inputs"]) == 3

def test_web_crawler_find_text_inputs():
    """Test filtering forms with text inputs"""
    processor = WebCrawlerProcessor()

    forms = [
        {
            "action": "/login",
            "inputs": [
                {"type": "text", "name": "username"},
                {"type": "password", "name": "password"}
            ]
        },
        {
            "action": "/submit",
            "inputs": [
                {"type": "submit", "value": "Go"}
            ]
        }
    ]

    text_input_forms = processor._filter_text_input_forms(forms)
    assert len(text_input_forms) == 1
    assert text_input_forms[0]["action"] == "/login"
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/workflows/test_web_crawler.py -v`

Expected: FAIL

**Step 3: Write web crawler processor**

Create: `app/workflows/processors/web_crawler.py`

```python
"""Web crawler for detecting input fields and forms"""
from typing import Dict, Any, List
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

class WebCrawlerProcessor:
    """Processor for crawling websites and detecting input fields"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Security Scanner) AppleWebKit/537.36'
        })

    def execute(self, task, previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute web crawling task

        Args:
            task: WorkflowTask with parameters:
                - source_task: Task ID containing URLs/subdomains
                - source_field: Field containing URLs
                - max_depth: Maximum crawl depth (default: 2)
                - max_pages: Maximum pages per domain (default: 50)
                - timeout: Request timeout in seconds (default: 10)
            previous_results: Previous task results

        Returns:
            Dictionary with pages_with_inputs list
        """
        try:
            params = task.parameters

            # Get source URLs
            source_task = params.get("source_task")
            source_field = params.get("source_field", "urls")

            if source_task not in previous_results:
                return {"success": False, "error": "Source task not found"}

            source_data = previous_results[source_task].get(source_field, [])

            # Ensure URLs have scheme
            urls = []
            for item in source_data:
                if isinstance(item, dict):
                    url = item.get("url") or f"https://{item.get('name', '')}"
                else:
                    url = item if item.startswith('http') else f"https://{item}"
                urls.append(url)

            # Crawl configuration
            max_depth = params.get("max_depth", 2)
            max_pages = params.get("max_pages", 50)
            timeout = params.get("timeout", 10)

            # Crawl each URL
            pages_with_inputs = []

            for base_url in urls:
                try:
                    crawled = self._crawl_site(
                        base_url,
                        max_depth=max_depth,
                        max_pages=max_pages,
                        timeout=timeout
                    )
                    pages_with_inputs.extend(crawled)
                except Exception as e:
                    continue  # Skip failed domains

            return {
                "success": True,
                "pages_with_inputs": pages_with_inputs,
                "total_pages": len(pages_with_inputs)
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _crawl_site(self, base_url: str, max_depth: int, max_pages: int, timeout: int) -> List[Dict]:
        """Crawl a single site and find forms with text inputs"""
        visited = set()
        to_visit = [(base_url, 0)]  # (url, depth)
        pages_with_inputs = []

        base_domain = urlparse(base_url).netloc

        while to_visit and len(visited) < max_pages:
            url, depth = to_visit.pop(0)

            if url in visited or depth > max_depth:
                continue

            visited.add(url)

            try:
                # Fetch page
                response = self.session.get(url, timeout=timeout, verify=False, allow_redirects=True)

                if response.status_code != 200:
                    continue

                # Parse HTML
                forms = self._parse_forms_from_html(response.text, url)

                # Filter forms with text inputs
                text_input_forms = self._filter_text_input_forms(forms)

                if text_input_forms:
                    pages_with_inputs.append({
                        "url": url,
                        "forms": text_input_forms
                    })

                # Extract links for further crawling
                if depth < max_depth:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        next_url = urljoin(url, link['href'])
                        next_domain = urlparse(next_url).netloc

                        # Stay within same domain
                        if next_domain == base_domain and next_url not in visited:
                            to_visit.append((next_url, depth + 1))

                time.sleep(0.5)  # Rate limiting

            except Exception:
                continue

        return pages_with_inputs

    def _parse_forms_from_html(self, html: str, base_url: str) -> List[Dict]:
        """Parse forms from HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        forms = []

        for form in soup.find_all('form'):
            action = form.get('action', '')
            method = form.get('method', 'get').lower()

            # Resolve relative URLs
            full_action = urljoin(base_url, action)

            # Extract inputs
            inputs = []
            for input_tag in form.find_all(['input', 'textarea']):
                input_type = input_tag.get('type', 'text')
                input_name = input_tag.get('name', '')

                inputs.append({
                    "type": input_type,
                    "name": input_name,
                    "value": input_tag.get('value', '')
                })

            forms.append({
                "action": full_action,
                "method": method,
                "inputs": inputs
            })

        return forms

    def _filter_text_input_forms(self, forms: List[Dict]) -> List[Dict]:
        """Filter forms that have text/password inputs"""
        text_types = {'text', 'password', 'email', 'search', 'tel', 'url'}

        filtered = []
        for form in forms:
            has_text_input = any(
                inp.get('type') in text_types or inp.get('type') == 'textarea'
                for inp in form.get('inputs', [])
            )
            if has_text_input:
                filtered.append(form)

        return filtered
```

**Step 4: Install required dependency**

Add to `requirements.txt`:
```
beautifulsoup4==4.12.3
```

Run: `pip install beautifulsoup4==4.12.3`

**Step 5: Run test to verify it passes**

Run: `pytest tests/workflows/test_web_crawler.py -v`

Expected: PASS

**Step 6: Commit**

```bash
git add app/workflows/processors/web_crawler.py tests/workflows/test_web_crawler.py requirements.txt
git commit -m "feat: add web crawler for detecting input fields

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 6: Exploit Database Lookup Integration

**Files:**
- Create: `app/workflows/processors/exploit_lookup.py`
- Create: `tests/workflows/test_exploit_lookup.py`

**Step 1: Write failing test for exploit lookup**

Create: `tests/workflows/test_exploit_lookup.py`

```python
import pytest
from app.workflows.processors.exploit_lookup import ExploitLookupProcessor

def test_exploit_lookup_parse_service_version():
    """Test parsing service version strings"""
    processor = ExploitLookupProcessor()

    service = "Apache httpd 2.4.49"
    parsed = processor._parse_service_version(service)

    assert parsed["product"] == "Apache httpd"
    assert parsed["version"] == "2.4.49"

def test_exploit_lookup_filter_safe_exploits():
    """Test filtering out dangerous exploit types"""
    processor = ExploitLookupProcessor()

    exploits = [
        {"id": "CVE-2021-1234", "type": "remote code execution", "description": "RCE vuln"},
        {"id": "CVE-2021-5678", "type": "denial of service", "description": "DoS vuln"},
        {"id": "CVE-2021-9999", "type": "social engineering", "description": "Phishing"},
    ]

    safe = processor._filter_safe_exploits(exploits)

    assert len(safe) == 1
    assert safe[0]["id"] == "CVE-2021-1234"
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/workflows/test_exploit_lookup.py -v`

Expected: FAIL

**Step 3: Write exploit lookup processor**

Create: `app/workflows/processors/exploit_lookup.py`

```python
"""Exploit database lookup for vulnerability matching"""
from typing import Dict, Any, List
import requests
import re
import json
from pathlib import Path

class ExploitLookupProcessor:
    """Processor for looking up exploits from service fingerprints"""

    DANGEROUS_TYPES = {
        'dos', 'denial of service', 'ddos',
        'social engineering', 'phishing', 'spam'
    }

    def __init__(self):
        self.exploitdb_api = "https://www.exploit-db.com/search"
        self.cve_api = "https://services.nvd.nist.gov/rest/json/cves/2.0"
        self.session = requests.Session()

    def execute(self, task, previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute exploit lookup task

        Args:
            task: WorkflowTask with parameters:
                - source_task: Task containing service fingerprints
                - source_field: Field with service data (default: "services")
                - include_experimental: Include experimental exploits (default: False)
            previous_results: Previous task results

        Returns:
            Dictionary with matched_exploits list
        """
        try:
            params = task.parameters
            source_task = params.get("source_task")
            source_field = params.get("source_field", "services")

            if source_task not in previous_results:
                return {"success": False, "error": "Source task not found"}

            services = previous_results[source_task].get(source_field, [])

            # Look up exploits for each service
            matched_exploits = []

            for service in services:
                service_name = service.get("service") or service.get("name", "")
                version = service.get("version", "")

                if not service_name:
                    continue

                # Query exploit databases
                exploits = self._query_exploits(service_name, version)

                # Filter out dangerous exploit types
                safe_exploits = self._filter_safe_exploits(exploits)

                if safe_exploits:
                    matched_exploits.append({
                        "service": service_name,
                        "version": version,
                        "host": service.get("host", ""),
                        "port": service.get("port", ""),
                        "exploits": safe_exploits
                    })

            return {
                "success": True,
                "matched_exploits": matched_exploits,
                "total_matches": len(matched_exploits)
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _query_exploits(self, service: str, version: str) -> List[Dict]:
        """Query multiple exploit sources"""
        exploits = []

        # Try local ExploitDB cache first
        local_results = self._query_local_exploitdb(service, version)
        exploits.extend(local_results)

        # Try CVE database
        try:
            cve_results = self._query_nvd_cve(service, version)
            exploits.extend(cve_results)
        except Exception:
            pass  # CVE lookup failed, continue

        return exploits

    def _query_local_exploitdb(self, service: str, version: str) -> List[Dict]:
        """Query local ExploitDB cache (if available)"""
        # Check for local exploitdb clone
        exploitdb_path = Path("/usr/share/exploitdb/files_exploits.csv")

        if not exploitdb_path.exists():
            return []

        matches = []
        search_term = f"{service} {version}".lower()

        try:
            with open(exploitdb_path, 'r', encoding='latin1') as f:
                for line in f:
                    if search_term in line.lower():
                        parts = line.split(',')
                        if len(parts) >= 3:
                            matches.append({
                                "id": parts[0],
                                "description": parts[2],
                                "type": parts[1] if len(parts) > 1 else "unknown",
                                "source": "exploitdb"
                            })
        except Exception:
            pass

        return matches[:10]  # Limit results

    def _query_nvd_cve(self, service: str, version: str) -> List[Dict]:
        """Query NVD CVE database"""
        matches = []

        try:
            # Build search query
            keyword = f"{service} {version}"

            params = {
                "keywordSearch": keyword,
                "resultsPerPage": 10
            }

            response = self.session.get(
                self.cve_api,
                params=params,
                timeout=10
            )

            if response.status_code == 200:
                data = response.json()

                for vuln in data.get("vulnerabilities", []):
                    cve = vuln.get("cve", {})

                    matches.append({
                        "id": cve.get("id", ""),
                        "description": cve.get("descriptions", [{}])[0].get("value", ""),
                        "cvss_score": self._extract_cvss(cve),
                        "source": "nvd"
                    })

        except Exception:
            pass  # API request failed

        return matches

    def _extract_cvss(self, cve: Dict) -> float:
        """Extract CVSS score from CVE data"""
        try:
            metrics = cve.get("metrics", {})
            cvss_v3 = metrics.get("cvssMetricV31", [{}])[0]
            return cvss_v3.get("cvssData", {}).get("baseScore", 0.0)
        except Exception:
            return 0.0

    def _parse_service_version(self, service_string: str) -> Dict[str, str]:
        """Parse service and version from string like 'Apache httpd 2.4.49'"""
        # Try to match pattern: "Product Version"
        match = re.search(r'(.+?)\s+([\d.]+)', service_string)

        if match:
            return {
                "product": match.group(1).strip(),
                "version": match.group(2).strip()
            }

        return {"product": service_string, "version": ""}

    def _filter_safe_exploits(self, exploits: List[Dict]) -> List[Dict]:
        """Filter out DoS, social engineering, and other dangerous exploit types"""
        safe = []

        for exploit in exploits:
            exploit_type = exploit.get("type", "").lower()
            description = exploit.get("description", "").lower()

            # Check if exploit type or description contains dangerous keywords
            is_dangerous = any(
                danger in exploit_type or danger in description
                for danger in self.DANGEROUS_TYPES
            )

            if not is_dangerous:
                safe.append(exploit)

        return safe
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/workflows/test_exploit_lookup.py -v`

Expected: PASS

**Step 5: Commit**

```bash
git add app/workflows/processors/exploit_lookup.py tests/workflows/test_exploit_lookup.py
git commit -m "feat: add exploit database lookup processor

Filters out DoS and social engineering exploits

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 7: JSON Result Aggregator

**Files:**
- Create: `app/workflows/processors/json_aggregator.py`
- Create: `tests/workflows/test_json_aggregator.py`

**Step 1: Write failing test for JSON aggregator**

Create: `tests/workflows/test_json_aggregator.py`

```python
import pytest
import json
from pathlib import Path
from app.workflows.processors.json_aggregator import JsonAggregatorProcessor
from app.workflows.schemas import WorkflowTask, TaskType

def test_json_aggregator_basic(tmp_path):
    """Test basic JSON aggregation"""
    processor = JsonAggregatorProcessor()

    task = WorkflowTask(
        task_id="aggregate_results",
        name="Aggregate Results",
        task_type=TaskType.JSON_AGGREGATE,
        parameters={
            "output_file": str(tmp_path / "final_results.json"),
            "sections": [
                {"name": "subdomains", "source_task": "merge_subdomains", "source_field": "merged_data"},
                {"name": "ports", "source_task": "scan_masscan", "source_field": "hosts"}
            ]
        }
    )

    previous_results = {
        "merge_subdomains": {
            "merged_data": [
                {"name": "www.example.com", "ips": ["1.1.1.1"]}
            ]
        },
        "scan_masscan": {
            "hosts": [
                {"ip": "1.1.1.1", "ports": [{"port": 80}]}
            ]
        }
    }

    result = processor.execute(task, previous_results)

    assert result["success"] == True
    assert (tmp_path / "final_results.json").exists()

    # Load and verify JSON
    with open(tmp_path / "final_results.json") as f:
        data = json.load(f)

    assert "subdomains" in data
    assert "ports" in data
    assert len(data["subdomains"]) == 1
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/workflows/test_json_aggregator.py -v`

Expected: FAIL

**Step 3: Write JSON aggregator processor**

Create: `app/workflows/processors/json_aggregator.py`

```python
"""JSON result aggregator for final workflow output"""
from typing import Dict, Any, List
import json
from pathlib import Path
from datetime import datetime

class JsonAggregatorProcessor:
    """Processor for aggregating workflow results into final JSON report"""

    def execute(self, task, previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute JSON aggregation task

        Args:
            task: WorkflowTask with parameters:
                - output_file: Path to output JSON file
                - sections: List of sections to include, each with:
                    - name: Section name in output JSON
                    - source_task: Task ID to pull data from
                    - source_field: Field in source task results
                    - optional: If True, skip if source not found (default: False)
                - include_metadata: Include metadata section (default: True)
            previous_results: Previous task results

        Returns:
            Dictionary with success status and output file path
        """
        try:
            params = task.parameters
            output_file = Path(params.get("output_file"))
            sections_config = params.get("sections", [])
            include_metadata = params.get("include_metadata", True)

            # Build aggregated result
            aggregated = {}

            # Add metadata
            if include_metadata:
                aggregated["metadata"] = {
                    "generated_at": datetime.utcnow().isoformat(),
                    "workflow_id": task.task_id,
                    "total_sections": len(sections_config)
                }

            # Add each configured section
            for section_config in sections_config:
                section_name = section_config.get("name")
                source_task = section_config.get("source_task")
                source_field = section_config.get("source_field")
                optional = section_config.get("optional", False)

                if source_task not in previous_results:
                    if not optional:
                        return {
                            "success": False,
                            "error": f"Required source task '{source_task}' not found"
                        }
                    continue

                source_data = previous_results[source_task]

                if source_field not in source_data:
                    if not optional:
                        return {
                            "success": False,
                            "error": f"Field '{source_field}' not found in task '{source_task}'"
                        }
                    continue

                aggregated[section_name] = source_data[source_field]

            # Write to file
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w') as f:
                json.dump(aggregated, f, indent=2)

            return {
                "success": True,
                "output_file": str(output_file),
                "sections_written": len(aggregated) - (1 if include_metadata else 0)
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/workflows/test_json_aggregator.py -v`

Expected: PASS

**Step 5: Commit**

```bash
git add app/workflows/processors/json_aggregator.py tests/workflows/test_json_aggregator.py
git commit -m "feat: add JSON aggregator for final workflow results

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Phase 4: Workflow Engine Integration

### Task 8: Update WorkflowEngine to Support New Task Types

**Files:**
- Modify: `app/workflows/engine.py:174-180`

**Step 1: Write test for new task type execution**

Create: `tests/workflows/test_engine_processors.py`

```python
import pytest
from app.workflows.engine import WorkflowWorker
from app.workflows.schemas import WorkflowDefinition, WorkflowTask, TaskType

def test_engine_executes_file_output_task(tmp_path):
    """Test that engine can execute FILE_OUTPUT tasks"""
    workflow = WorkflowDefinition(
        workflow_id="test_file_output",
        name="Test File Output",
        target="example.com",
        tasks=[
            WorkflowTask(
                task_id="dummy_source",
                name="Dummy Source",
                tool="echo",  # Hypothetical
                task_type=TaskType.TOOL,
                parameters={}
            ),
            WorkflowTask(
                task_id="save_output",
                name="Save Output",
                task_type=TaskType.FILE_OUTPUT,
                parameters={
                    "source_task": "dummy_source",
                    "source_field": "data",
                    "output_file": str(tmp_path / "output.txt")
                },
                depends_on=["dummy_source"]
            )
        ]
    )

    # Execution test would require mocking tool execution
    # This is a placeholder for structure validation
    assert workflow.tasks[1].task_type == TaskType.FILE_OUTPUT
```

**Step 2: Run test to verify current behavior**

Run: `pytest tests/workflows/test_engine_processors.py -v`

Expected: PASS (structure validation)

**Step 3: Update WorkflowEngine to handle new task types**

Modify: `app/workflows/engine.py`

Find the `_execute_task` method around line 174 and update:

```python
def _execute_task(self, task_def) -> bool:
    """Execute a single task (tool, merge, file_output, web_crawl, etc.)"""
    # Import processors
    from app.workflows.processors.file_output import FileOutputProcessor
    from app.workflows.processors.web_crawler import WebCrawlerProcessor
    from app.workflows.processors.exploit_lookup import ExploitLookupProcessor
    from app.workflows.processors.json_aggregator import JsonAggregatorProcessor

    # Route to appropriate handler
    if task_def.task_type == TaskType.MERGE:
        return self._execute_merge_task(task_def)
    elif task_def.task_type == TaskType.FILE_OUTPUT:
        return self._execute_processor_task(task_def, FileOutputProcessor())
    elif task_def.task_type == TaskType.WEB_CRAWL:
        return self._execute_processor_task(task_def, WebCrawlerProcessor())
    elif task_def.task_type == TaskType.EXPLOIT_LOOKUP:
        return self._execute_processor_task(task_def, ExploitLookupProcessor())
    elif task_def.task_type == TaskType.JSON_AGGREGATE:
        return self._execute_processor_task(task_def, JsonAggregatorProcessor())
    else:  # TaskType.TOOL
        return self._execute_tool_task(task_def)
```

**Step 4: Add generic processor execution method**

Add new method to WorkflowWorker class in `app/workflows/engine.py`:

```python
def _execute_processor_task(self, task_def, processor) -> bool:
    """Execute a custom processor task"""
    task_logger = get_workflow_logger(
        scan_id=self.scan_id,
        task_id=task_def.task_id
    )

    task_logger.info(f"Starting processor task: {task_def.name}")
    self.task_started.emit(task_def.task_id, task_def.name)

    # Create task record
    db = SessionLocal()
    task_record = Task(
        scan_id=self.scan_id,
        task_name=task_def.name,
        tool=task_def.task_type,  # Use task type as "tool"
        status="running",
        parameters=json.dumps(task_def.parameters)
    )
    db.add(task_record)
    db.commit()
    task_id_db = task_record.id
    db.close()

    try:
        # Execute processor
        start_time = time.time()
        result = processor.execute(task_def, {
            task_id: result.output
            for task_id, result in self.task_results.items()
        })
        execution_time = time.time() - start_time

        success = result.get("success", False)

        # Create task result
        task_result = TaskResult(
            task_id=task_def.task_id,
            status=TaskStatus.COMPLETED if success else TaskStatus.FAILED,
            output=result,
            execution_time=execution_time
        )

        self.task_results[task_def.task_id] = task_result

        # Update database
        db = SessionLocal()
        task_record = db.query(Task).filter(Task.id == task_id_db).first()
        task_record.status = "completed" if success else "failed"
        task_record.output = json.dumps(result)
        task_record.completed_at = datetime.utcnow()
        db.commit()
        db.close()

        if success:
            task_logger.info(f"Processor task completed successfully in {execution_time:.2f}s")
            self.task_completed.emit(task_def.task_id, result)
        else:
            error = result.get("error", "Unknown error")
            task_logger.error(f"Processor task failed: {error}")
            self.task_failed.emit(task_def.task_id, error)

        return success

    except Exception as e:
        task_logger.exception(f"Processor task failed with exception: {e}")

        db = SessionLocal()
        task_record = db.query(Task).filter(Task.id == task_id_db).first()
        task_record.status = "failed"
        task_record.errors = str(e)
        task_record.completed_at = datetime.utcnow()
        db.commit()
        db.close()

        self.task_failed.emit(task_def.task_id, str(e))
        return False
```

**Step 5: Test updated engine**

Run: `pytest tests/workflows/ -v`

Expected: All workflow tests PASS

**Step 6: Commit**

```bash
git add app/workflows/engine.py tests/workflows/test_engine_processors.py
git commit -m "feat: integrate custom processors into workflow engine

Supports FILE_OUTPUT, WEB_CRAWL, EXPLOIT_LOOKUP, JSON_AGGREGATE

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Phase 5: Service Fingerprinting Adapter

### Task 9: Nmap Service Detection Adapter Enhancement

**Files:**
- Modify: `app/tools/adapters/nmap_adapter.py`

**Step 1: Read current Nmap adapter**

Run: `cat app/tools/adapters/nmap_adapter.py`

**Step 2: Enhance parse_output to extract service versions**

Modify the `parse_output` method in `app/tools/adapters/nmap_adapter.py` to extract service names and versions:

```python
def parse_output(self, output: str, stderr: str, return_code: int) -> Dict[str, Any]:
    """Parse nmap XML output into structured data with service fingerprints"""
    try:
        import xml.etree.ElementTree as ET
        root = ET.fromstring(output)

        hosts = []
        services = []  # NEW: Collect all services for fingerprinting

        for host in root.findall('.//host'):
            # ... existing host parsing code ...

            # Parse ports and extract service versions
            for port in host.findall('.//port'):
                port_id = port.get('portid')
                protocol = port.get('protocol', 'tcp')

                state_elem = port.find('state')
                state = state_elem.get('state') if state_elem is not None else 'unknown'

                service_elem = port.find('service')
                service_name = ''
                service_version = ''
                service_product = ''

                if service_elem is not None:
                    service_name = service_elem.get('name', '')
                    service_product = service_elem.get('product', '')
                    service_version = service_elem.get('version', '')

                port_data = {
                    'port': int(port_id),
                    'protocol': protocol,
                    'state': state,
                    'service': service_name,
                    'product': service_product,
                    'version': service_version
                }

                host_ports.append(port_data)

                # Add to services list for fingerprinting
                if service_name and state == 'open':
                    services.append({
                        'host': ip_addr,
                        'port': int(port_id),
                        'service': service_product or service_name,
                        'version': service_version,
                        'full_string': f"{service_product} {service_version}".strip()
                    })

            # ... rest of host parsing ...

        return {
            'hosts': hosts,
            'services': services,  # NEW: Return services for exploit lookup
            'total_hosts': len(hosts),
            'total_services': len(services)
        }

    except Exception as e:
        # Fallback to text parsing
        return {'hosts': [], 'services': [], 'error': str(e)}
```

**Step 3: Test enhanced Nmap adapter**

Run: `pytest tests/tools/test_nmap_adapter.py -v`

Expected: PASS (may need to update tests to expect 'services' field)

**Step 4: Commit**

```bash
git add app/tools/adapters/nmap_adapter.py
git commit -m "feat: enhance Nmap adapter to extract service fingerprints

Enables service version detection for exploit lookup

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Phase 6: Complete Workflow Definition

### Task 10: Build Advanced Recon & Exploitation Workflow

**Files:**
- Create: `app/workflows/prebuilt/advanced_recon_exploit.py`
- Modify: `app/workflows/prebuilt/__init__.py`

**Step 1: Write workflow definition**

Create: `app/workflows/prebuilt/advanced_recon_exploit.py`

```python
"""Advanced reconnaissance and exploitation workflow"""
from app.workflows.schemas import WorkflowDefinition, WorkflowTask, TaskType

def create_advanced_recon_exploit_workflow(domain: str) -> WorkflowDefinition:
    """
    Comprehensive automated penetration testing workflow:
    - Multi-tool subdomain enumeration with deduplication
    - IP extraction and high-speed port scanning
    - Service fingerprinting and exploit lookup
    - Directory enumeration and sensitive file detection
    - Web crawling for input fields
    - Automated SQL injection testing
    - Final JSON report aggregation
    """

    return WorkflowDefinition(
        workflow_id=f"advanced_recon_exploit_{domain}",
        name="Advanced Recon & Exploitation",
        description="Full-spectrum automated penetration test with intelligent exploitation",
        target=domain,
        tasks=[
            # ============================================================
            # Phase 1: Subdomain Enumeration
            # ============================================================

            WorkflowTask(
                task_id="enum_amass",
                name="Amass Enumeration (ASNs, IPs, Subdomains)",
                description="Deep reconnaissance with Amass for ASN/IP/subdomain discovery",
                tool="amass",
                parameters={
                    "domain": domain,
                    "passive": True
                },
                priority=10,
                timeout=900
            ),

            # Save Amass subdomains to file
            WorkflowTask(
                task_id="save_amass_subdomains",
                name="Save Amass Subdomains to File",
                task_type=TaskType.FILE_OUTPUT,
                parameters={
                    "source_task": "enum_amass",
                    "source_field": "subdomains",
                    "extract_field": "name",
                    "output_file": f"data/scans/{domain}/lists/subdomains_amass.txt"
                },
                depends_on=["enum_amass"],
                priority=9
            ),

            # Save Amass IPs to file
            WorkflowTask(
                task_id="save_amass_ips",
                name="Save Amass IPs to File",
                task_type=TaskType.FILE_OUTPUT,
                parameters={
                    "source_task": "enum_amass",
                    "source_field": "subdomains",
                    "extract_field": "ips",
                    "output_file": f"data/scans/{domain}/lists/ips_amass.txt"
                },
                depends_on=["enum_amass"],
                priority=9
            ),

            # Run Subfinder
            WorkflowTask(
                task_id="enum_subfinder",
                name="Subfinder Enumeration",
                tool="subfinder",
                parameters={
                    "domain": domain,
                    "all": True,
                    "resolve": True
                },
                priority=10,
                timeout=600
            ),

            # Run Sublist3r
            WorkflowTask(
                task_id="enum_sublist3r",
                name="Sublist3r Enumeration",
                tool="sublist3r",
                parameters={
                    "domain": domain,
                    "verbose": False
                },
                priority=10,
                timeout=600
            ),

            # Merge all subdomain sources
            WorkflowTask(
                task_id="merge_all_subdomains",
                name="Merge & Deduplicate All Subdomains",
                description="Combine Amass, Subfinder, Sublist3r results with IP merging",
                task_type=TaskType.MERGE,
                merge_sources=["enum_amass", "enum_subfinder", "enum_sublist3r"],
                merge_field="subdomains",
                dedupe_key="name",
                merge_strategy="combine",
                depends_on=["enum_amass", "enum_subfinder", "enum_sublist3r"],
                priority=8
            ),

            # Save final subdomain list
            WorkflowTask(
                task_id="save_final_subdomains",
                name="Save Final Subdomain List",
                task_type=TaskType.FILE_OUTPUT,
                parameters={
                    "source_task": "merge_all_subdomains",
                    "source_field": "merged_data",
                    "extract_field": "name",
                    "output_file": f"data/scans/{domain}/lists/subdomains_final.txt"
                },
                depends_on=["merge_all_subdomains"],
                priority=7
            ),

            # Extract all unique IPs
            WorkflowTask(
                task_id="save_final_ips",
                name="Save Final IP List",
                task_type=TaskType.FILE_OUTPUT,
                parameters={
                    "source_task": "merge_all_subdomains",
                    "source_field": "merged_data",
                    "extract_field": "ips",
                    "output_file": f"data/scans/{domain}/lists/ips_final.txt"
                },
                depends_on=["merge_all_subdomains"],
                priority=7
            ),

            # ============================================================
            # Phase 2: Port Scanning
            # ============================================================

            # Masscan on discovered IPs
            WorkflowTask(
                task_id="scan_masscan",
                name="Masscan Port & Service Discovery",
                description="High-speed port scan on all discovered IPs",
                tool="masscan",
                parameters={
                    "targets": "${merge_all_subdomains.ips}",  # Reference merged IPs
                    "ports": "21,22,25,80,443,445,3306,3389,5432,8080,8443",
                    "rate": 10000
                },
                depends_on=["merge_all_subdomains"],
                priority=6,
                timeout=1800
            ),

            # ============================================================
            # Phase 3: Service Fingerprinting
            # ============================================================

            # Nmap service detection on open ports
            WorkflowTask(
                task_id="fingerprint_services",
                name="Nmap Service Version Detection",
                description="Detailed service fingerprinting for version numbers",
                tool="nmap",
                parameters={
                    "hosts": "${scan_masscan.ips}",
                    "ports": "${scan_masscan.ports}",
                    "scan_type": "service_version",
                    "service_detection": True,
                    "version_intensity": 9
                },
                depends_on=["scan_masscan"],
                priority=5,
                timeout=3600
            ),

            # ============================================================
            # Phase 4: Exploit Lookup
            # ============================================================

            WorkflowTask(
                task_id="lookup_exploits",
                name="Exploit Database Lookup",
                description="Find exploits matching discovered service versions",
                task_type=TaskType.EXPLOIT_LOOKUP,
                parameters={
                    "source_task": "fingerprint_services",
                    "source_field": "services",
                    "include_experimental": False
                },
                depends_on=["fingerprint_services"],
                priority=4
            ),

            # Start final JSON aggregation
            WorkflowTask(
                task_id="aggregate_recon_results",
                name="Aggregate Reconnaissance Results",
                description="Create JSON report with all recon data",
                task_type=TaskType.JSON_AGGREGATE,
                parameters={
                    "output_file": f"data/scans/{domain}/results/recon_results.json",
                    "sections": [
                        {"name": "subdomains", "source_task": "merge_all_subdomains", "source_field": "merged_data"},
                        {"name": "ports", "source_task": "scan_masscan", "source_field": "hosts"},
                        {"name": "services", "source_task": "fingerprint_services", "source_field": "services"},
                        {"name": "exploits", "source_task": "lookup_exploits", "source_field": "matched_exploits"}
                    ]
                },
                depends_on=["lookup_exploits"],
                priority=3
            ),

            # ============================================================
            # Phase 5: Directory Enumeration
            # ============================================================

            WorkflowTask(
                task_id="enum_directories",
                name="FFUF Directory Enumeration",
                description="Discover directories on all subdomains",
                tool="ffuf",
                parameters={
                    "urls": "${merge_all_subdomains.urls}",
                    "wordlist": "/usr/share/wordlists/dirb/common.txt",
                    "extensions": "php,html,js,txt,xml,json",
                    "follow_redirects": True
                },
                depends_on=["merge_all_subdomains"],
                priority=3,
                timeout=3600
            ),

            # ============================================================
            # Phase 6: Web Crawling
            # ============================================================

            WorkflowTask(
                task_id="crawl_for_inputs",
                name="Crawl for Text Input Fields",
                description="Crawl all subdomains to find forms with text inputs",
                task_type=TaskType.WEB_CRAWL,
                parameters={
                    "source_task": "merge_all_subdomains",
                    "source_field": "merged_data",
                    "max_depth": 2,
                    "max_pages": 50,
                    "timeout": 10
                },
                depends_on=["merge_all_subdomains"],
                priority=2
            ),

            # Save pages with input fields
            WorkflowTask(
                task_id="save_input_pages",
                name="Save Input Field Pages List",
                task_type=TaskType.FILE_OUTPUT,
                parameters={
                    "source_task": "crawl_for_inputs",
                    "source_field": "pages_with_inputs",
                    "output_file": f"data/scans/{domain}/lists/pages_with_inputs.json",
                    "format": "json"
                },
                depends_on=["crawl_for_inputs"],
                priority=2
            ),

            # ============================================================
            # Phase 7: SQL Injection Testing
            # ============================================================

            WorkflowTask(
                task_id="test_sql_injection",
                name="SQLMap Automated Testing",
                description="Test all input field pages for SQL injection",
                tool="sqlmap",
                parameters={
                    "urls": "${crawl_for_inputs.pages_with_inputs}",
                    "batch": True,
                    "level": 2,
                    "risk": 2,
                    "forms": True
                },
                depends_on=["crawl_for_inputs"],
                priority=1,
                timeout=7200,
                optional=True  # Don't fail workflow if SQLMap fails
            ),

            # ============================================================
            # Phase 8: Final Aggregation
            # ============================================================

            WorkflowTask(
                task_id="final_json_report",
                name="Generate Final JSON Report",
                description="Aggregate all results into comprehensive report",
                task_type=TaskType.JSON_AGGREGATE,
                parameters={
                    "output_file": f"data/scans/{domain}/results/final_report.json",
                    "sections": [
                        {"name": "subdomains", "source_task": "merge_all_subdomains", "source_field": "merged_data"},
                        {"name": "ips", "source_task": "save_final_ips", "source_field": "items_written", "optional": True},
                        {"name": "ports", "source_task": "scan_masscan", "source_field": "hosts"},
                        {"name": "services", "source_task": "fingerprint_services", "source_field": "services"},
                        {"name": "exploits", "source_task": "lookup_exploits", "source_field": "matched_exploits"},
                        {"name": "directories", "source_task": "enum_directories", "source_field": "results", "optional": True},
                        {"name": "input_pages", "source_task": "crawl_for_inputs", "source_field": "pages_with_inputs"},
                        {"name": "sql_injection", "source_task": "test_sql_injection", "source_field": "findings", "optional": True},
                        {"name": "login_pages", "source_task": "crawl_for_inputs", "source_field": "login_pages", "optional": True}
                    ],
                    "include_metadata": True
                },
                depends_on=["aggregate_recon_results", "test_sql_injection"],
                priority=0
            )
        ],

        stop_on_failure=False,  # Continue even if some tasks fail
        max_parallel_tasks=1  # Sequential execution for safety
    )
```

**Step 2: Register workflow in factory**

Modify: `app/workflows/prebuilt/__init__.py`

```python
from app.workflows.prebuilt.advanced_recon_exploit import create_advanced_recon_exploit_workflow

class WorkflowFactory:
    @staticmethod
    def create_workflow(workflow_type: str, target: str) -> WorkflowDefinition:
        workflows = {
            # ... existing workflows ...
            "advanced_recon_exploit": create_advanced_recon_exploit_workflow
        }

        # ... rest of factory logic ...

    @staticmethod
    def list_workflows() -> List[Dict[str, str]]:
        return [
            # ... existing workflows ...
            {
                "id": "advanced_recon_exploit",
                "name": "Advanced Recon & Exploitation",
                "description": "Full-spectrum automated penetration test"
            }
        ]
```

**Step 3: Test workflow validation**

Run: `python -c "from app.workflows.prebuilt.advanced_recon_exploit import create_advanced_recon_exploit_workflow; w = create_advanced_recon_exploit_workflow('example.com'); print('Valid workflow:', w.name)"`

Expected: Output "Valid workflow: Advanced Recon & Exploitation"

**Step 4: Commit**

```bash
git add app/workflows/prebuilt/advanced_recon_exploit.py app/workflows/prebuilt/__init__.py
git commit -m "feat: add advanced recon & exploitation workflow

Complete automated pentest workflow with:
- Multi-tool subdomain enumeration
- Port scanning with Masscan
- Service fingerprinting
- Exploit database lookup
- Directory enumeration
- Web crawling for inputs
- SQL injection testing
- Comprehensive JSON reporting

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Phase 7: Documentation and Next Steps

### Task 11: Update Documentation

**Files:**
- Modify: `CLAUDE.md`
- Create: `docs/ADVANCED_WORKFLOW.md`

**Step 1: Create workflow documentation**

Create: `docs/ADVANCED_WORKFLOW.md`

```markdown
# Advanced Reconnaissance & Exploitation Workflow

## Overview

The Advanced Recon & Exploitation workflow is a comprehensive automated penetration testing workflow that performs:

1. **Multi-tool subdomain enumeration** (Amass, Subfinder, Sublist3r)
2. **Intelligent result deduplication** with IP merging
3. **High-speed port scanning** (Masscan)
4. **Service fingerprinting** (Nmap version detection)
5. **Exploit database lookup** (CVE/ExploitDB matching)
6. **Directory enumeration** (FFUF)
7. **Web crawling** for input field detection
8. **Automated SQL injection testing** (SQLMap)
9. **Comprehensive JSON report** generation

## Usage

```python
from app.workflows.prebuilt import WorkflowFactory

# Create workflow
workflow = WorkflowFactory.create_workflow("advanced_recon_exploit", "example.com")

# Execute via GUI or programmatically
from app.workflows.engine import WorkflowWorker
worker = WorkflowWorker(workflow, user_id=1)
worker.start()
```

## Output Files

All results are saved to `data/scans/{domain}/`:

- `lists/subdomains_final.txt` - Deduplicated subdomain list
- `lists/ips_final.txt` - Unique IP addresses
- `lists/pages_with_inputs.json` - Pages with text input fields
- `results/recon_results.json` - Reconnaissance phase results
- `results/final_report.json` - Complete penetration test results

## Workflow Phases

### Phase 1: Subdomain Enumeration (Priority 10-7)
- Runs Amass, Subfinder, Sublist3r in parallel (priority 10)
- Merges and deduplicates results (priority 8)
- Saves subdomain/IP lists to files (priority 7)

### Phase 2: Port Scanning (Priority 6)
- Masscan scans all discovered IPs
- Focuses on common service ports

### Phase 3: Service Fingerprinting (Priority 5)
- Nmap version detection on open ports
- Extracts service names and versions

### Phase 4: Exploit Lookup (Priority 4)
- Queries CVE/ExploitDB for matching exploits
- Filters out DoS and social engineering exploits

### Phase 5: Directory Enumeration (Priority 3)
- FFUF discovers hidden directories
- Scans all subdomains with common wordlist

### Phase 6: Web Crawling (Priority 2)
- Crawls each subdomain for forms with text inputs
- Identifies potential SQL injection points

### Phase 7: SQL Injection Testing (Priority 1)
- SQLMap tests all input field pages
- Automated vulnerability detection

### Phase 8: Final Report (Priority 0)
- Aggregates all results into comprehensive JSON
- Includes metadata and execution statistics

## Safety Features

- **No destructive exploits**: DoS and social engineering filtered
- **Optional tasks**: SQLMap marked optional to prevent workflow failure
- **Sequential execution**: Tasks run one at a time for safety
- **Timeout limits**: All tasks have reasonable timeout values
- **Error handling**: Workflow continues even if individual tasks fail

## Required Tools

All tools must be installed and in PATH:

- amass
- subfinder
- sublist3r
- masscan (requires root)
- nmap
- ffuf
- sqlmap

Run `python check_tools.py` to verify installation.

## Next Steps

After workflow completion:

1. **Review final_report.json** for discovered vulnerabilities
2. **Manually test** SQL injection findings flagged by SQLMap
3. **Investigate** matched exploits for applicable vulnerabilities
4. **Document** login pages found during crawling
5. **Follow up** on high-value findings with manual testing
```

**Step 2: Update CLAUDE.md**

Append to `CLAUDE.md`:

```markdown
## Advanced Workflow: Reconnaissance & Exploitation

A new comprehensive workflow `advanced_recon_exploit` is available that performs:

**Subdomain enumeration  Deduplication  Port scanning  Service fingerprinting  Exploit lookup  Directory enumeration  Input field detection  SQL injection testing  JSON reporting**

See `docs/ADVANCED_WORKFLOW.md` for detailed documentation.

**Key features:**
- Multi-tool subdomain discovery with intelligent merging
- Automated exploit matching (excludes DoS/social engineering)
- Web crawling for input field detection
- SQL injection vulnerability testing
- Comprehensive JSON report generation

**New Task Types:**
- `FILE_OUTPUT`: Save results to text/JSON files
- `WEB_CRAWL`: Crawl websites for forms/inputs
- `EXPLOIT_LOOKUP`: Query exploit databases
- `JSON_AGGREGATE`: Generate final JSON reports

**New Tool Adapters:**
- `sublist3r`: Subdomain enumeration via search engines
- `masscan`: High-speed port scanner

**New Utilities:**
- `app/utils/result_utils.py`: Deduplication and file I/O helpers
- `app/workflows/processors/`: Custom task processors for advanced operations
```

**Step 3: Commit documentation**

```bash
git add docs/ADVANCED_WORKFLOW.md CLAUDE.md
git commit -m "docs: add advanced workflow documentation

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Summary and Recommendations

### What Comes Next?

After implementing this workflow, consider:

1. **Manual Testing Integration**
   - Add GUI button to mark findings for "manual review"
   - Create task type `MANUAL_TEST_COORDINATOR` that pauses workflow and prompts user
   - Save manual test notes to database

2. **Sensitive File Analysis**
   - Create processor that analyzes downloaded files for:
     - Credentials in JavaScript/config files
     - Additional subdomains in HTML/JS
     - API endpoints and tokens
   - Use regex patterns and AST parsing

3. **Login Page Detection Enhancement**
   - Add logic to `WebCrawlerProcessor` to identify login pages
   - Look for password inputs, "login" in URL/form action
   - Extract and categorize by subdomain

4. **Automated Exploitation**
   - Create `EXPLOIT_EXECUTOR` task type
   - Use Metasploit adapter for matched exploits
   - Require user confirmation before execution
   - Log all exploitation attempts

5. **Reporting Enhancements**
   - PDF report generation from final JSON
   - Executive summary with risk scoring
   - Timeline of workflow execution
   - Screenshots of vulnerabilities

6. **Performance Optimization**
   - Enable parallel task execution for independent tasks
   - Implement task result caching
   - Add resume capability for interrupted workflows

7. **Additional Tool Integrations**
   - Nikto for web server scanning
   - Hydra for password brute forcing (with authorization)
   - Shodan API for external reconnaissance
   - GitHub dorking for leaked credentials

### Architecture Notes

This implementation follows YAGNI and DRY principles:
- Processors are single-purpose and reusable
- File I/O utilities are centralized
- Workflow engine routes to appropriate handlers
- Task types are validated via Pydantic schemas

All exploit lookup and execution explicitly excludes:
- Denial of Service (DoS/DDoS)
- Social engineering attacks
- Destructive techniques

This maintains the platform's defensive security purpose.
